# A-Visual-Attention-Grounding-Neural-Model
## 1. Overview
This code repository contains the implementation of our research project: _A Visual Attention Grounding Neural Model for Multimodal Machine Translation._

We introduce a novel multimodal machine translation model that utilizes parallel visual and texture information. Our model jointly optimizes learning of a shared visual-language embedding and translating languages. It does this with the aid of a visual attention grounding mechanism which links the visual semantics in the image with the corresponding textual se-
mantics.

![An Overview of the Visual Attention NMT](https://github.com/zmykevin/A-Visual-Attention-Grounding-Neural-Model/blob/master/AGV-NMT.jpg)
